{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install requests pandas\n",
        "import pandas as pd\n",
        "from itertools import combinations"
      ],
      "metadata": {
        "id": "kNZhfKljeR9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8L7vbTu-d9wi"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"file.csv\", on_bad_lines=\"skip\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df = df[df['authorships.countries'].str.contains('US')]\n"
      ],
      "metadata": {
        "id": "SeLe6XhTeF7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "important_cols = [\n",
        "    'id', 'type', 'type_crossref', 'cited_by_count', 'referenced_works_count',\n",
        "    'publication_year', 'authorships.author.id', 'authorships.countries'\n",
        "]\n",
        "\n",
        "filtered_df = filtered_df[important_cols].copy()  # Ensuring a copy to avoid modifying the original DataFrame\n"
      ],
      "metadata": {
        "id": "MaJ6Kw9BeMyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "\n",
        "def get_author_country(openalex_id):\n",
        "    \"\"\"Fetch country code for a given author ID from OpenAlex API.\"\"\"\n",
        "    url = f\"https://api.openalex.org/authors/{openalex_id}\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        data = response.json()\n",
        "        if 'affiliations' in data and data['affiliations']:\n",
        "            return data['affiliations'][0]['institution'].get('country_code', 'Unknown')\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {openalex_id}: {e}\")\n",
        "    return \"Unknown\"\n",
        "\n",
        "def fill_missing_countries(row):\n",
        "    \"\"\"Identify and fill missing country codes for a row in DataFrame.\"\"\"\n",
        "    author_ids = str(row['authorships.author.id']).split(\"|\")  # Convert NaN to empty string\n",
        "    country_codes = str(row['authorships.countries']).split(\"|\")\n",
        "\n",
        "    # Ensure both lists are the same length\n",
        "    while len(country_codes) < len(author_ids):\n",
        "        country_codes.append(\"\")\n",
        "\n",
        "    for i in range(len(author_ids)):\n",
        "        if not country_codes[i]:  # Check for empty or None\n",
        "            openalex_id = author_ids[i].split(\"/\")[-1]  # Extract author ID\n",
        "            country_codes[i] = get_author_country(openalex_id) or \"Unknown\"  # Ensure it's a string\n",
        "\n",
        "    return \"|\".join(map(str, country_codes))  # Convert all items to string before joining\n",
        "\n",
        "\n",
        "# Fill NaN values with empty string\n",
        "filtered_df = filtered_df.fillna(\"\")\n",
        "\n",
        "# Apply function with progress bar\n",
        "tqdm.pandas()\n",
        "filtered_df[\"updated_authorships.countries\"] = filtered_df.progress_apply(fill_missing_countries, axis=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "nvOJ16_CeWFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import permutations\n",
        "import pandas as pd\n",
        "\n",
        "# Function to process author pairs (now includes backward pairs)\n",
        "def create_author_pairs(row):\n",
        "    if pd.isna(row['authorships.author.id']):  # Handle missing values\n",
        "        return [(None, None, None, None)]\n",
        "\n",
        "    authors = [a.replace(\"https://openalex.org/\", \"\") for a in row['authorships.author.id'].split(\"|\")]\n",
        "    countries = row['updated_authorships.countries'].split(\"|\") if pd.notna(row['updated_authorships.countries']) else [\"Unknown\"] * len(authors)\n",
        "\n",
        "    # If only one author, return (Author1, Country1, None, None)\n",
        "    if len(authors) == 1:\n",
        "        return [(authors[0], countries[0], None, None)]\n",
        "\n",
        "    # Generate all ordered author pairs (including backward pairs)\n",
        "    author_pairs = list(permutations(zip(authors, countries), 2))\n",
        "\n",
        "    # Format output\n",
        "    return [(a1[0], a1[1], a2[0], a2[1]) for a1, a2 in author_pairs]\n",
        "\n",
        "# Apply function and explode into multiple rows\n",
        "filtered_df['author_pairs'] = filtered_df.apply(create_author_pairs, axis=1)\n",
        "filtered_df = filtered_df.explode('author_pairs')\n",
        "\n",
        "# Convert tuple values into separate columns\n",
        "filtered_df[['Author ID1', 'Author ID1 Country', 'Author ID2', 'Author ID2 Country']] = pd.DataFrame(filtered_df['author_pairs'].tolist(), index=filtered_df.index)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "filtered_df = filtered_df.drop(columns=['author_pairs', 'authorships.author.id', 'updated_authorships.countries'])\n",
        "\n",
        "# Reset index\n",
        "filtered_df = filtered_df.reset_index(drop=True)\n",
        "\n",
        "# Display result\n",
        "print(filtered_df)\n"
      ],
      "metadata": {
        "id": "pFrg40_yeZyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = filtered_df[filtered_df[\"Author ID1 Country\"] == \"US\"]\n"
      ],
      "metadata": {
        "id": "zO5nyoXCeccM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.to_excel(\"final.xlsx\", index=False)\n"
      ],
      "metadata": {
        "id": "_gCUDWmseefa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXTRACTING PROFILES FROM TWITTER FOR AUTHOR ID-1\n"
      ],
      "metadata": {
        "id": "B76gdJHhfR79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# spliting the authorid from the link\n",
        "\n",
        "author_tweetid = pd.read_csv(r'Input\\authors_tweeters_2024_02.csv')\n",
        "print(f'The shape of the input data: {author_tweetid.shape}')\n",
        "\n",
        "\n",
        "author_tweetid['author_id_extracted'] = author_tweetid['author_id'].str.extract(r'https://openalex.org/(.+)')\n",
        "author_tweetid['tweeter_id']= author_tweetid['tweeter_id'].astype(str)\n",
        "print(f'The shape of the output data: {author_tweetid.shape}')\n",
        "author_tweetid.to_csv(\"authors_tweeters_updated_file.csv\", index=False)"
      ],
      "metadata": {
        "id": "4kP0e5w8fXpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Excel file into a DataFrame\n",
        "left_file_path = r'merged_file.csv' # Replace with your actual file path\n",
        "left_df = pd.read_csv(left_file_path, dtype= str)\n",
        "# left_df = left_df.dropna(subset=['Author ID1'])\n",
        "# left_df = left_df.drop_duplicates(subset=['Author ID1'])\n",
        "# left_df['tweeter_id'] = left_df['tweeter_id'].astype(str)\n",
        "print(f'The shape of left dataframe: {left_df.shape}')\n",
        "\n",
        "right_filepath = r'Output_merged_full.xlsx'\n",
        "right_df = pd.read_excel(right_filepath, dtype= str)\n",
        "# right_df['User ID'] = right_df['User ID'].astype(str)\n",
        "\n",
        "left_df_merged = left_df\n",
        "left_df_merged =  left_df_merged.merge(right_df, left_on=\"tweeter_id\", right_on=\"User ID\", how=\"left\",suffixes=('', '_right'), indicator=True)\n",
        "left_df_merged['Is_Merged'] = left_df_merged['_merge'] == 'both'\n",
        "\n",
        "\n",
        "columns_to_drop = [col for col in left_df_merged.columns if col.endswith('_right')]\n",
        "left_df_merged = left_df_merged.drop(columns=columns_to_drop)\n",
        "left_df_merged['tweeter_id'] = left_df_merged['tweeter_id'].astype(str)\n",
        "left_df_merged['User ID']= left_df_merged['User ID'].astype(str)\n",
        "left_df_merged.to_excel(r'Final_Merged.xlsx', index=False)\n",
        "print(f'The shape of the output file: {left_df_merged.shape}')"
      ],
      "metadata": {
        "id": "U4LQfuPbfeQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import config\n",
        "from datetime import datetime\n",
        "\n",
        "# Initialize the Tweepy Client\n",
        "client = tweepy.Client(bearer_token=config.Bearer_token)\n",
        "\n",
        "# Load the Excel file containing usernames\n",
        "input_file_path = r'Twitter_Profile\\Input_data_for_scraping_userprofiles\\merged_file.xlsx'\n",
        "\n",
        "# Save the cleaned data\n",
        "cleaned_file_path = \"Twitter_Profile\\Output_Scraped_data\\Cleaned_Twitter_Scraped_Data_Profiles_1000_New_test_3.xlsx\"\n",
        "twitter_handles_df = pd.read_excel(input_file_path).head(20)\n",
        "\n",
        "# Clean the usernames: Remove leading/trailing whitespaces and drop blanks\n",
        "# twitter_handles_df['username'] = twitter_handles_df['username'].str.strip()\n",
        "twitter_handles_df = twitter_handles_df[twitter_handles_df['username'].notna()]\n",
        "\n",
        "# Prepare a list to store scraped data\n",
        "scraped_data = []\n",
        "\n",
        "# Counter for partial file versions\n",
        "partial_file_counter = 1\n",
        "\n",
        "# Function to monitor rate limits\n",
        "def check_rate_limit(response):\n",
        "    remaining = int(response.headers.get('x-rate-limit-remaining', 1))\n",
        "    reset_time = int(response.headers.get('x-rate-limit-reset', time.time() + 60))\n",
        "    return remaining, reset_time\n",
        "\n",
        "# Use tqdm for a progress bar\n",
        "for username in tqdm(twitter_handles_df['username'], desc=\"Scraping Twitter Data\"):\n",
        "    # username = username.strip()  # Ensure no leading or trailing spaces\n",
        "    while True:\n",
        "        try:\n",
        "            user = client.get_user(\n",
        "                # username=username,\n",
        "                id = username,\n",
        "                user_fields=[\n",
        "                    'id', 'name', 'username', 'created_at', 'description',\n",
        "                    'public_metrics', 'location', 'entities', 'verified',\n",
        "                    'profile_image_url', 'protected', 'profile_banner_url'\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            # Handle missing user data\n",
        "            if user.data is None:\n",
        "                print(f\"[WARNING] No data found for username: {username}. Skipping.\")\n",
        "                scraped_data.append({\n",
        "                    'User ID': username,\n",
        "                    'Name': 'Not Found',\n",
        "                    'Username': 'Not Found',\n",
        "                    'Profile URL': f\"https://twitter.com/i/user/{username}\",\n",
        "                    'Profile Image URL': 'Not Found',\n",
        "                    'Profile Banner URL': 'Not Found',\n",
        "                    'Protected Status': 'Not Found',\n",
        "                    'Description': 'Not Found',\n",
        "                    'Location': 'Not Found',\n",
        "                    'Followers Count': 'Not Found',\n",
        "                    'Following Count': 'Not Found',\n",
        "                    'Tweet Count': 'Not Found',\n",
        "                    'Media Count': 'Not Found',\n",
        "                    'Listed Count': 'Not Found',\n",
        "                    'Account Creation Date': 'Not Found',\n",
        "                    'Verified Status': 'Not Found',\n",
        "                    'External Link in Bio': 'Not Found'\n",
        "                })\n",
        "                break  # Skip to the next username\n",
        "\n",
        "            external_link = 'N/A'\n",
        "            if user.data.entities:\n",
        "                if 'url' in user.data.entities and 'urls' in user.data.entities['url']:\n",
        "                    urls = user.data.entities['url']['urls']\n",
        "                    if len(urls) > 0 and 'expanded_url' in urls[0]:\n",
        "                        external_link = urls[0]['expanded_url']\n",
        "\n",
        "            # Construct profile URL\n",
        "            profile_url = f\"https://twitter.com/i/user/{user.data.id}\"\n",
        "\n",
        "            # Extract data\n",
        "            profile_image_url = user.data.profile_image_url if hasattr(user.data, 'profile_image_url') else 'N/A'\n",
        "            profile_banner_url = user.data.profile_banner_url if hasattr(user.data, 'profile_banner_url') else 'N/A'\n",
        "            listed_count = user.data.public_metrics.get('listed_count', 'N/A')\n",
        "            media_count = user.data.public_metrics.get('media_count', 'N/A')  # line to retrieve media count\n",
        "            protected_status = user.data.protected\n",
        "\n",
        "            scraped_data.append({\n",
        "                'User ID': user.data.id,\n",
        "                'Name': user.data.name,\n",
        "                'Username': user.data.username,\n",
        "                'Profile URL': profile_url,\n",
        "                'Profile Image URL': profile_image_url,\n",
        "                'Profile Banner URL': profile_banner_url,\n",
        "                'Protected Status': protected_status,\n",
        "                'Description': user.data.description,\n",
        "                'Location': user.data.location if user.data.location else 'N/A',\n",
        "                'Followers Count': user.data.public_metrics['followers_count'],\n",
        "                'Following Count': user.data.public_metrics['following_count'],\n",
        "                'Tweet Count': user.data.public_metrics['tweet_count'],\n",
        "                'Media Count': media_count,  # Include the media count in the output\n",
        "                'Listed Count': listed_count,\n",
        "                'Account Creation Date': user.data.created_at,\n",
        "                'Verified Status': user.data.verified,\n",
        "                'External Link in Bio': external_link})\n",
        "\n",
        "            break  # Exit the while loop on successful fetch\n",
        "        except tweepy.TooManyRequests as e:\n",
        "            # Save progress to a local CSV file with incrementing filename\n",
        "            partial_output_file = f'Partial_Scraped_data\\Twitter_Scraped_Data_Partial_german_users{partial_file_counter}.csv'\n",
        "            pd.DataFrame(scraped_data).to_csv(partial_output_file, index=False, encoding='utf-8-sig')\n",
        "            print(f\"[PROGRESS SAVED] Partial data saved locally to {partial_output_file}\")\n",
        "            partial_file_counter = partial_file_counter + 1  # Increment the counter for the next file\n",
        "\n",
        "            # Handle rate limit exception\n",
        "            remaining, reset_time = check_rate_limit(e.response)\n",
        "            sleep_time = max(0, reset_time - time.time())\n",
        "            print(f\"[PAUSE] Rate limit reached. Sleeping for {sleep_time} seconds...\")\n",
        "            time.sleep(sleep_time)\n",
        "            print(\"[RESUME] Resuming data scraping...\")\n",
        "        except tweepy.TweepyException as e:\n",
        "            # Log other errors and continue with the next username\n",
        "            print(f\"[ERROR] Failed to fetch data for username: {username}. Error: {e}\")\n",
        "            scraped_data.append({\n",
        "                'User ID': username,\n",
        "                'Name': 'Error',\n",
        "                'Username': 'Error',\n",
        "                'Profile URL': f\"https://twitter.com/i/user/{username}\",\n",
        "                'Profile Image URL': 'Error',\n",
        "                'Profile Banner URL': 'Error',\n",
        "                'Protected Status': 'Error',\n",
        "                'Description': 'Error',\n",
        "                'Location': 'Error',\n",
        "                'Followers Count': 'Error',\n",
        "                'Following Count': 'Error',\n",
        "                'Tweet Count': 'Error',\n",
        "                'Media Count': 'Error',  # Add media count to error case\n",
        "                'Listed Count': 'Error',\n",
        "                'Account Creation Date': 'Error',\n",
        "                'Verified Status': 'Error',\n",
        "                'External Link in Bio': str(e)\n",
        "            })\n",
        "            break  # Skip to the next username on other errors\n",
        "\n",
        "# Convert the list of scraped data to a DataFrame\n",
        "scraped_data_df = pd.DataFrame(scraped_data)\n",
        "scraped_data_df['Account Creation Date'] = pd.to_datetime(scraped_data_df['Account Creation Date'], errors='coerce')\n",
        "scraped_data_df['Account Creation Date'] = scraped_data_df['Account Creation Date'].dt.strftime('%a %b %d %H:%M:%S %z %Y')\n",
        "# Save the DataFrame to a final CSV file\n",
        "output_file_path = 'Partial_Scraped_data\\Twitter_Scraped_Data_userprofiles.csv'\n",
        "scraped_data_df.to_csv(output_file_path, index=False, encoding='utf-8-sig')\n",
        "print(f\"Final scraped data has been saved locally to {output_file_path}\")\n",
        "\n",
        "# Cleaning the data\n",
        "# Load the data\n",
        "data = pd.read_csv(output_file_path)\n",
        "\n",
        "# Function to clean string columns by stripping leading/trailing whitespace and collapsing multiple spaces\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    else:\n",
        "        # Strip leading/trailing whitespace and replace multiple spaces with a single space\n",
        "        return ' '.join(text.strip().split())\n",
        "\n",
        "# Apply the cleaning function to all string columns in the dataframe\n",
        "string_columns = data.select_dtypes(include=['object']).columns\n",
        "data_cleaned = data.copy()\n",
        "for column in string_columns:\n",
        "    data_cleaned[column] = data[column].apply(clean_text)\n",
        "\n",
        "data_cleaned[\"User ID\"] = data_cleaned['User ID'].astype(str)\n",
        "\n",
        "data_cleaned.to_excel(cleaned_file_path, index=False)\n",
        "\n",
        "print(f\"Final scraped data has been cleaned and saved locally to {cleaned_file_path}\")\n"
      ],
      "metadata": {
        "id": "VfULPPlBfe_Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}